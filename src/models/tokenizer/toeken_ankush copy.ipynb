{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users/ankushroy/iris/src', '/users/ankushroy/iris/src/models/tokenizer', '/users/ankushroy/anaconda3/envs/myenv_exp_1/lib/python38.zip', '/users/ankushroy/anaconda3/envs/myenv_exp_1/lib/python3.8', '/users/ankushroy/anaconda3/envs/myenv_exp_1/lib/python3.8/lib-dynload', '', '/users/ankushroy/anaconda3/envs/myenv_exp_1/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/users/ankushroy/iris/src')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Credits to https://github.com/CompVis/taming-transformers\n",
    "\"\"\"\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Tuple\n",
    "\n",
    "\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from dataset import Batch\n",
    "from lpips import LPIPS\n",
    "from nets import Encoder, Decoder\n",
    "from utils import LossWithIntermediateLosses\n",
    "\n",
    "\n",
    "batch=1\n",
    "@dataclass\n",
    "class TokenizerEncoderOutput:\n",
    "    z: torch.FloatTensor\n",
    "    z_quantized: torch.FloatTensor\n",
    "    tokens: torch.LongTensor\n",
    "\n",
    "\n",
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, encoder: Encoder, decoder: Decoder, with_lpips: bool) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoder = encoder\n",
    "        self.pre_quant_conv = torch.nn.Conv2d(encoder.config.z_channels, embed_dim, 1)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, decoder.config.z_channels, 1)\n",
    "        self.decoder = decoder\n",
    "        self.embedding.weight.data.uniform_(-1.0 / vocab_size, 1.0 / vocab_size)\n",
    "        self.lpips = LPIPS().eval() if with_lpips else None\n",
    "\n",
    "        # EMA attributes\n",
    "        # self.decay = decay\n",
    "        # self.register_buffer('ema_cluster_size', torch.zeros(vocab_size))\n",
    "        # self.register_buffer('ema_w', self.embedding.weight.clone())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"tokenizer\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor, should_preprocess: bool = False, should_postprocess: bool = False) -> Tuple[torch.Tensor]:\n",
    "        outputs = self.encode(x, should_preprocess)\n",
    "        decoder_input = outputs.z + (outputs.z_quantized - outputs.z).detach()\n",
    "        reconstructions = self.decode(decoder_input, should_postprocess)\n",
    "        return outputs.z, outputs.z_quantized, reconstructions\n",
    "\n",
    "    def compute_loss(self, x, **kwargs: Any) -> LossWithIntermediateLosses:\n",
    "        \n",
    "        if self.training:\n",
    "            self.embedding.weight.data.copy_(self.ema_w)\n",
    "        \n",
    "        assert self.lpips is not None\n",
    "        observations = x\n",
    "        z, z_quantized, reconstructions = self(observations, should_preprocess=False, should_postprocess=False)\n",
    "\n",
    "        # Codebook loss. Notes:\n",
    "        # - beta position is different from taming and identical to original VQVAE paper\n",
    "        # - VQVAE uses 0.25 by default\n",
    "        beta = 1.0\n",
    "        commitment_loss = (z.detach() - z_quantized).pow(2).mean() + beta * (z - z_quantized.detach()).pow(2).mean()\n",
    "\n",
    "        reconstruction_loss = torch.abs(observations - reconstructions).mean()\n",
    "        perceptual_loss = torch.mean(self.lpips(observations, reconstructions))\n",
    "\n",
    "        return LossWithIntermediateLosses(commitment_loss=commitment_loss, reconstruction_loss=reconstruction_loss,perceptual_loss=perceptual_loss)\n",
    "\n",
    "    def encode(self, x: torch.Tensor, should_preprocess: bool = False) -> TokenizerEncoderOutput:\n",
    "        if should_preprocess:\n",
    "            x = self.preprocess_input(x)\n",
    "        #print(\"Shape of x:\", x.shape)\n",
    "        shape = x.shape  # (..., C, H, W)\n",
    "        x = x.view(-1, *shape[-3:])\n",
    "        #print(\"Shape of x as (x_view):\", x.shape)\n",
    "        z = self.encoder(x)\n",
    "        #print(\"Shape of z:\",z.shape)\n",
    "        z = self.pre_quant_conv(z)\n",
    "        b, e, h, w = z.shape\n",
    "        z_flattened = rearrange(z, 'b e h w -> (b h w) e')\n",
    "        #print(\"Shape of z_flattend:\",z_flattened.shape)\n",
    "        dist_to_embeddings = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + torch.sum(self.embedding.weight**2, dim=1) - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "\n",
    "        tokens = dist_to_embeddings.argmin(dim=-1)\n",
    "        #print(\"Shape of tokens:\",tokens.shape)\n",
    "        z_q = rearrange(self.embedding(tokens), '(b h w) e -> b e h w', b=b, e=e, h=h, w=w).contiguous()\n",
    "        #print(\"Shape of z_q:\",z_q.shape)\n",
    "\n",
    "        # EMA update\n",
    "        # if self.training:\n",
    "        #     one_hot = torch.zeros(tokens.shape[0], self.vocab_size).to(tokens.device)\n",
    "        #     one_hot.scatter_(1, tokens.view(-1, 1), 1)\n",
    "        #     n_total = one_hot.sum(dim=0)\n",
    "        #     ema_cluster_size = self.decay * self.ema_cluster_size + (1 - self.decay) * n_total\n",
    "        #     w_avg = torch.sum(one_hot.T @ z_flattened, dim=0)\n",
    "        #     n = torch.clamp(ema_cluster_size, min=1e-5)\n",
    "        #     ema_w = w_avg / n.unsqueeze(-1)\n",
    "        #     self.ema_cluster_size = ema_cluster_size\n",
    "        #     self.ema_w = self.decay * self.ema_w + (1 - self.decay) * ema_w\n",
    "\n",
    "        # Reshape to original\n",
    "        z = z.reshape(*shape[:-3], *z.shape[1:])\n",
    "        #print(\"Shape of reshaped z:\", z.shape)\n",
    "        z_q = z_q.reshape(*shape[:-3], *z_q.shape[1:])\n",
    "        #print(\"Shape of reshaped z_q:\", z_q.shape)\n",
    "        tokens = tokens.reshape(*shape[:-3], -1)\n",
    "        #print(\"Shape of tokens:\", tokens.shape)\n",
    "\n",
    "        return TokenizerEncoderOutput(z, z_q, tokens)\n",
    "\n",
    "    def decode(self, z_q: torch.Tensor, should_postprocess: bool = False) -> torch.Tensor:\n",
    "        shape = z_q.shape  # (..., E, h, w)\n",
    "        z_q = z_q.view(-1, *shape[-3:])\n",
    "        z_q = self.post_quant_conv(z_q)\n",
    "        rec = self.decoder(z_q)\n",
    "        rec = rec.reshape(*shape[:-3], *rec.shape[1:])\n",
    "        if should_postprocess:\n",
    "            rec = self.postprocess_output(rec)\n",
    "        return rec\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_decode(self, x: torch.Tensor, should_preprocess: bool = False, should_postprocess: bool = False) -> torch.Tensor:\n",
    "        z_q = self.encode(x, should_preprocess).z_quantized\n",
    "        return self.decode(z_q, should_postprocess)\n",
    "\n",
    "    def preprocess_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"x is supposed to be channels first and in [0, 1]\"\"\"\n",
    "        return x.mul(2).sub(1)\n",
    "\n",
    "    def postprocess_output(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"y is supposed to be channels first and in [-1, 1]\"\"\"\n",
    "        return y.add(1).div(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "## Configuration files \n",
    "@dataclass\n",
    "class EncoderDecoderConfig:\n",
    "    resolution: int\n",
    "    in_channels: int\n",
    "    z_channels: int\n",
    "    ch: int\n",
    "    ch_mult: List[int]\n",
    "    num_res_blocks: int\n",
    "    attn_resolutions: List[int]\n",
    "    out_ch: int\n",
    "    dropout: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder(EncoderDecoderConfig(resolution=256,\n",
    "#                                        in_channels=1,\n",
    "#                                         z_channels=1024,\n",
    "#                                         ch=128,\n",
    "#                                         ch_mult= [1, 1, 1, 2, 2, 4],\n",
    "#                                         num_res_blocks= 2,\n",
    "#                                         attn_resolutions= [8],\n",
    "#                                         out_ch= 1,\n",
    "#                                         dropout= 0.0))\n",
    "# decoder=Decoder(EncoderDecoderConfig(resolution=256,\n",
    "#                                        in_channels=1,\n",
    "#                                         z_channels=1024,\n",
    "#                                         ch=128,\n",
    "#                                         ch_mult= [1, 1, 1, 2, 2, 4],\n",
    "#                                         num_res_blocks= 2,\n",
    "#                                         attn_resolutions= [8],  \n",
    "#                                         out_ch= 1,\n",
    "#                                         dropout= 0.0))\n",
    "# vocab_size = 1024 # actual vocabulary size \n",
    "# embed_dim = 1024  # the desired embedding dimension of the codebook (coebook dim)\n",
    "# tokenizer = Tokenizer(vocab_size, embed_dim, encoder, decoder, with_lpips=True)\n",
    "# print(encoder)\n",
    "# print(decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer : shape of latent is (256, 8, 8).\n",
      "Encoder(\n",
      "  (conv_in): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (down): ModuleList(\n",
      "    (0): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList()\n",
      "      (downsample): Downsample(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (1): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList()\n",
      "      (downsample): Downsample(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (2): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList()\n",
      "      (downsample): Downsample(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (3): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList()\n",
      "      (downsample): Downsample(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (4): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList(\n",
      "        (0): AttnBlock(\n",
      "          (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): AttnBlock(\n",
      "          (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (downsample): Downsample(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (5): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList()\n",
      "    )\n",
      "  )\n",
      "  (mid): Module(\n",
      "    (block_1): ResnetBlock(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (attn_1): AttnBlock(\n",
      "      (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (block_2): ResnetBlock(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv_out): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Decoder(\n",
      "  (conv_in): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (mid): Module(\n",
      "    (block_1): ResnetBlock(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (attn_1): AttnBlock(\n",
      "      (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (block_2): ResnetBlock(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (up): ModuleList(\n",
      "    (0): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList()\n",
      "    )\n",
      "    (1): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList()\n",
      "      (upsample): Upsample(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList()\n",
      "      (upsample): Upsample(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (3): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList()\n",
      "      (upsample): Upsample(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (4): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList(\n",
      "        (0): AttnBlock(\n",
      "          (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): AttnBlock(\n",
      "          (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): AttnBlock(\n",
      "          (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsample): Upsample(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): Module(\n",
      "      (block): ModuleList(\n",
      "        (0): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (attn): ModuleList()\n",
      "      (upsample): Upsample(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (conv_out): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(EncoderDecoderConfig(resolution=256,\n",
    "                                       in_channels=1,\n",
    "                                        z_channels=256,\n",
    "                                        ch=128,\n",
    "                                        ch_mult= [1, 1, 1, 2, 2, 4],\n",
    "                                        num_res_blocks= 2,\n",
    "                                        attn_resolutions= [16],\n",
    "                                        out_ch= 1,\n",
    "                                        dropout= 0.2))\n",
    "decoder=Decoder(EncoderDecoderConfig(resolution=256,\n",
    "                                       in_channels=1,\n",
    "                                        z_channels=256,\n",
    "                                        ch=128,\n",
    "                                        ch_mult= [1, 1, 1, 2, 2, 4],\n",
    "                                        num_res_blocks= 2,\n",
    "                                        attn_resolutions= [16],\n",
    "                                        out_ch= 1,\n",
    "                                        dropout= 0.2))\n",
    "vocab_size = 1024 # actual vocabulary size \n",
    "embed_dim = 256  # the desired embedding dimension of the codebook (coebook dim)\n",
    "tokenizer = Tokenizer(vocab_size, embed_dim, encoder, decoder, with_lpips=True)\n",
    "print(encoder)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0001 # cfg training file \n",
    "optimizer_tokenizer = torch.optim.Adam(tokenizer.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "import torch\n",
    "import sys\n",
    "from nuwa_pytorch import VQGanVAE\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "def eventGeneration(start_time, obs_time = 3 ,lead_time = 6, time_interval = 30):\n",
    "    # Generate event based on starting time point, return a list: [[t-4,...,t-1,t], [t+1,...,t+72]]\n",
    "    # Get the start year, month, day, hour, minute\n",
    "    year = int(start_time[0:4])\n",
    "    month = int(start_time[4:6])\n",
    "    day = int(start_time[6:8])\n",
    "    hour = int(start_time[8:10])\n",
    "    minute = int(start_time[10:12])\n",
    "    #print(datetime(year=year, month=month, day=day, hour=hour, minute=minute))\n",
    "    times = [(datetime(year, month, day, hour, minute) + timedelta(minutes=time_interval * (x+1))) for x in range(lead_time)]\n",
    "    lead = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    times = [(datetime(year, month, day, hour, minute) - timedelta(minutes=time_interval * x)) for x in range(obs_time)]\n",
    "    obs = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    obs.reverse()\n",
    "    return lead, obs\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor, Compose, CenterCrop\n",
    "class radarDataset(Dataset):\n",
    "    def __init__(self, root_dir, event_times, obs_number = 3, pred_number = 6, transform=None):\n",
    "        # event_times is an array of starting time t(string)\n",
    "        # transform is the preprocessing functions\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.event_times = event_times\n",
    "        self.obs_number = obs_number\n",
    "        self.pred_number = pred_number\n",
    "    def __len__(self):\n",
    "        return len(self.event_times)\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = str(self.event_times[idx])\n",
    "        time_list_pre, time_list_obs = eventGeneration(start_time, self.obs_number, self.pred_number)\n",
    "        output = []\n",
    "        time_list = time_list_obs + time_list_pre\n",
    "        #print(time_list)\n",
    "        for time in time_list:\n",
    "            year = time[0:4]\n",
    "            month = time[4:6]\n",
    "            #path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAC_MFBS_EM_5min_' + time + '_NL.h5'\n",
    "            path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAP_5min_' + time + '.h5'\n",
    "            image = np.array(h5py.File(path)['image1']['image_data'])\n",
    "            #image = np.ma.masked_where(image == 65535, image)\n",
    "            image = image[264:520,242:498]\n",
    "            image[image == 65535] = 0\n",
    "            image = image.astype('float32')\n",
    "            image = image/100*12\n",
    "            image = np.clip(image, 0, 128)\n",
    "            image = image/40\n",
    "            #image = 2*image-1 #normalize to [-1,1]\n",
    "            output.append(image)\n",
    "        output = torch.permute(torch.tensor(np.array(output)), (1, 2, 0))\n",
    "        output = self.transform(np.array(output))\n",
    "        return output\n",
    "#root_dir = '/users/hbi/data/RAD_NL25_RAC_MFBS_EM_5min/'\n",
    "#dataset = radarDataset(root_dir, [\"200808031600\"], transform = Compose([ToTensor(),CenterCrop(256)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30632 3493 3560\n"
     ]
    }
   ],
   "source": [
    "# develop dataset\n",
    "#from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "root_dir = '/home/hbi/RAD_NL25_RAP_5min/' \n",
    "batch_size=1\n",
    "\n",
    "df_train = pd.read_csv('/users/ankushroy/taming-transformers/training_Delfland08-14_20.csv', header = None)\n",
    "event_times = df_train[0].to_list()\n",
    "dataset_train = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_s = pd.read_csv('/users/ankushroy/taming-transformers/training_Delfland08-14.csv', header = None)\n",
    "event_times = df_train_s[0].to_list()\n",
    "dataset_train_del = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_test = pd.read_csv('/users/ankushroy/taming-transformers/testing_Delfland18-20.csv', header = None)\n",
    "event_times = df_test[0].to_list()\n",
    "dataset_test = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali = pd.read_csv('/users/ankushroy/taming-transformers/validation_Delfland15-17.csv', header = None)\n",
    "event_times = df_vali[0].to_list()\n",
    "dataset_vali = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_train_aa = pd.read_csv('/users/ankushroy/taming-transformers/training_Aa08-14.csv', header = None)\n",
    "event_times = df_train_aa[0].to_list()\n",
    "dataset_train_aa = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_dw = pd.read_csv('/users/ankushroy/taming-transformers/training_Dwar08-14.csv', header = None)\n",
    "event_times = df_train_dw[0].to_list()\n",
    "dataset_train_dw = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))    \n",
    "\n",
    "df_train_re = pd.read_csv('/users/ankushroy/taming-transformers/training_Regge08-14.csv', header = None)\n",
    "event_times = df_train_re[0].to_list()\n",
    "dataset_train_re = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))   \n",
    "\n",
    "data_list = [dataset_train_aa, dataset_train_dw, dataset_train_del, dataset_train_re]\n",
    "train_aadedwre = torch.utils.data.ConcatDataset(data_list)\n",
    "\n",
    "print(len(train_aadedwre), len(dataset_test), len(dataset_vali))\n",
    "#print(len(dataset_train_aa))\n",
    "loaders = { 'train' :DataLoader(train_aadedwre, batch_size, shuffle=True, num_workers=8),\n",
    "            'test' :DataLoader(dataset_test, batch_size, shuffle=False, num_workers=8), \n",
    "           'valid' :DataLoader(dataset_vali, batch_size, shuffle=False, num_workers=8),\n",
    "          \n",
    "          'train_aa5' :DataLoader(dataset_train_aa, batch_size, shuffle=False, num_workers=8),\n",
    "          'train_dw5' :DataLoader(dataset_train_dw, batch_size, shuffle=False, num_workers=8),\n",
    "          'train_del5' :DataLoader(dataset_train_del, batch_size, shuffle=True, num_workers=8),\n",
    "          'train_re5' :DataLoader(dataset_train_re, batch_size, shuffle=False, num_workers=8),\n",
    "          }\n",
    "\n",
    "#print(dataset_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda:0\")\n",
    "#     print(\"CUDA device is available\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"CUDA device is not available\")\n",
    "\n",
    "# print(\"Device:\", device)\n",
    "# print(\"PyTorch version:\", torch.__version__)\n",
    "# print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning the CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 11.3\n",
      "True\n",
      "cuda:0\n",
      "PyTorch version: 1.11.0\n"
     ]
    }
   ],
   "source": [
    "cuda_version = torch.version.cuda\n",
    "print(\"CUDA version:\", cuda_version)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# for images in (loaders['train']):\n",
    "#     image = images[0]\n",
    "#     image = image.unsqueeze(1)\n",
    "#     image = image[1:2, :, :, :]\n",
    "#     input_image= Variable(image).to('cpu')  # batch x\n",
    "#     #print(input_image.size())\n",
    "#     encoder_output = tokenizer.encode(input_image)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop for the Tokenizer (VQ-VAE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Losses: Total = 5.0618\n",
      "Losses: Total = 4.2520\n",
      "Losses: Total = 3.0779\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 47.54 GiB total capacity; 45.20 GiB already allocated; 10.81 MiB free; 45.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m#print(input_image.size())\u001b[39;00m\n\u001b[1;32m     27\u001b[0m encoder_output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(input_image)\n\u001b[0;32m---> 29\u001b[0m losses\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39;49mcompute_loss(input_image)\n\u001b[1;32m     31\u001b[0m loss_total_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mloss_total \n\u001b[1;32m     32\u001b[0m loss_total_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_total_step\u001b[39m.\u001b[39mitem()\u001b[39m/\u001b[39m\u001b[39m8\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m, in \u001b[0;36mTokenizer.compute_loss\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlpips \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m observations \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 60\u001b[0m z, z_quantized, reconstructions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(observations, should_preprocess\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, should_postprocess\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     62\u001b[0m \u001b[39m# Codebook loss. Notes:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m# - beta position is different from taming and identical to original VQVAE paper\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m# - VQVAE uses 0.25 by default\u001b[39;00m\n\u001b[1;32m     65\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_exp_1/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m, in \u001b[0;36mTokenizer.forward\u001b[0;34m(self, x, should_preprocess, should_postprocess)\u001b[0m\n\u001b[1;32m     48\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(x, should_preprocess)\n\u001b[1;32m     49\u001b[0m decoder_input \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mz \u001b[39m+\u001b[39m (outputs\u001b[39m.\u001b[39mz_quantized \u001b[39m-\u001b[39m outputs\u001b[39m.\u001b[39mz)\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m---> 50\u001b[0m reconstructions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(decoder_input, should_postprocess)\n\u001b[1;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\u001b[39m.\u001b[39mz, outputs\u001b[39m.\u001b[39mz_quantized, reconstructions\n",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m, in \u001b[0;36mTokenizer.decode\u001b[0;34m(self, z_q, should_postprocess)\u001b[0m\n\u001b[1;32m    117\u001b[0m z_q \u001b[39m=\u001b[39m z_q\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:])\n\u001b[1;32m    118\u001b[0m z_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_quant_conv(z_q)\n\u001b[0;32m--> 119\u001b[0m rec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(z_q)\n\u001b[1;32m    120\u001b[0m rec \u001b[39m=\u001b[39m rec\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m], \u001b[39m*\u001b[39mrec\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:])\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m should_postprocess:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_exp_1/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/iris/src/models/tokenizer/nets.py:191\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    189\u001b[0m             h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup[i_level]\u001b[39m.\u001b[39mattn[i_block](h)\n\u001b[1;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m i_level \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 191\u001b[0m         h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup[i_level]\u001b[39m.\u001b[39;49mupsample(h)\n\u001b[1;32m    193\u001b[0m \u001b[39m# end\u001b[39;00m\n\u001b[1;32m    194\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_out(h)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_exp_1/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/iris/src/models/tokenizer/nets.py:221\u001b[0m, in \u001b[0;36mUpsample.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 221\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49minterpolate(x, scale_factor\u001b[39m=\u001b[39;49m\u001b[39m2.0\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnearest\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_conv:\n\u001b[1;32m    223\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv_exp_1/lib/python3.8/site-packages/torch/nn/functional.py:3891\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3889\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mupsample_nearest1d(\u001b[39minput\u001b[39m, output_size, scale_factors)\n\u001b[1;32m   3890\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 3891\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mupsample_nearest2d(\u001b[39minput\u001b[39;49m, output_size, scale_factors)\n\u001b[1;32m   3892\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3893\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mupsample_nearest3d(\u001b[39minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 47.54 GiB total capacity; 45.20 GiB already allocated; 10.81 MiB free; 45.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training loop VQVAE \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "loss_total_step=0.0\n",
    "intermediate_losses = defaultdict(float)\n",
    "num_epochs=1\n",
    "device = torch.device(\"cuda:0\")  # Specify the GPU device\n",
    "epoch_start=0\n",
    "tokenizer.to(device)\n",
    "tokenizer.train()\n",
    "all_metrics = []  # List to store metrics for all epochs\n",
    "\n",
    "for epoch in range(epoch_start,num_epochs):\n",
    "    save_epoch = epoch in [0,2,4,9,19,29,39,49,59,79]\n",
    "    #save_epoch = epoch in [2]\n",
    "    loss_total_epoch=0.0\n",
    "    optimizer_tokenizer.zero_grad()\n",
    "    intermediate_losses = {}\n",
    "    print(\"epoch {}\".format(epoch))\n",
    "    for i, images in enumerate(loaders['train']):\n",
    "        loss_total_step=0.0\n",
    "        image = images[0]\n",
    "        image = image.unsqueeze(1)\n",
    "        image = image[3:4, :, :, :]\n",
    "        input_image= image.to(device)  # batch x\n",
    "        #print(input_image.size())\n",
    "        encoder_output = tokenizer.encode(input_image)\n",
    "        \n",
    "        losses=tokenizer.compute_loss(input_image)\n",
    "        \n",
    "        loss_total_step += losses.loss_total \n",
    "        loss_total_epoch += loss_total_step.item()/8\n",
    "        if (i+1) % 8 == 0:\n",
    "            (loss_total_step/8).backward()\n",
    "            optimizer_tokenizer.step()\n",
    "            optimizer_tokenizer.zero_grad()\n",
    "            \n",
    "            print(\"Losses: Total = {:.4f}\".format(loss_total_step.item()))\n",
    "            #loss_total_step=0.0\n",
    "        \n",
    "\n",
    "        for loss_name, loss_value in losses.intermediate_losses.items():\n",
    "                intermediate_losses[f\"{str(tokenizer)}/train/{loss_name}\"] = loss_value/8\n",
    "    \n",
    "        \n",
    "    metrics = {f'{str(Tokenizer)}/train/total_loss': loss_total_epoch, **intermediate_losses}\n",
    "    print(\"Epoch {}: Total Loss = {:.4f}\".format(epoch, metrics[f'{str(Tokenizer)}/train/total_loss']))\n",
    "\n",
    "    if save_epoch:\n",
    "        torch.save({\n",
    "        'model_state_dict': tokenizer.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_tokenizer.state_dict(),\n",
    "        }, '/space/ankushroy/Tokeniser_exp2_256_embdim_EMA/vqvae_checkpoint_epoch{}'.format(epoch+1))\n",
    "\n",
    "    all_metrics.append(metrics)  # Save metrics for the current epoch to the list\n",
    "\n",
    "\n",
    "# Convert the metrics list to a NumPy array\n",
    "metrics_array = np.array(all_metrics)\n",
    "\n",
    "# Save the metrics array as a NumPy file\n",
    "np.save('/space/ankushroy/Tokeniser_exp2_256_embdim_EMA/metrices.npy', metrics_array)\n",
    "# Print all errors from each epoch's metrics dictionary\n",
    "# print(all_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# metrics_array = np.array(all_metrics)\n",
    "# np.read('/space/ankushroy/Tokeniser_Epochs_Chkpts/metrices.npy', metrics_array)\n",
    "# print(metrics_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether the keys match with the loaded checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cuda:1\")\n",
    "#print(device)\n",
    "optimizer_checkpoint = torch.load('/space/ankushroy/Tokeniser_exp2_1024_embdim_5/vqvae_epoch5', map_location=device)\n",
    "\n",
    "# Print the keys in the optimizer_checkpoint dictionary\n",
    "print(optimizer_checkpoint.keys())\n",
    "\n",
    "# Adjust the key name based on the actual key in the optimizer_checkpoint dictionary\n",
    "optimizer_state_dict = optimizer_checkpoint['state']\n",
    "optimizer_tokenizer.load_state_dict(optimizer_checkpoint)\n",
    "\n",
    "# Check if the keys match\n",
    "loaded_keys = set(optimizer_checkpoint.keys())\n",
    "optimizer_keys = set(optimizer_tokenizer.state_dict().keys())\n",
    "\n",
    "if loaded_keys == optimizer_keys:\n",
    "    print(\"Keys match!\")\n",
    "else:\n",
    "    print(\"Keys do not match.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the metrics once the training is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the NumPy array from file\n",
    "array = np.load('/space/ankushroy/Tokeniser_Chkpts_recon_tokens_1024/metrices.npy', allow_pickle=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = array  # List of 79 metrics dictionaries\n",
    "\n",
    "# Create lists to store the loss values\n",
    "total_losses = []\n",
    "commitment_losses = []\n",
    "reconstruction_losses = []\n",
    "perceptual_losses = []\n",
    "\n",
    "# Extract the loss values from each metrics dictionary\n",
    "for metric in metrics:\n",
    "    total_losses.append(metric[\"<class '__main__.Tokenizer'>/train/total_loss\"])\n",
    "    commitment_losses.append(metric[\"tokenizer/train/commitment_loss\"])\n",
    "    reconstruction_losses.append(metric[\"tokenizer/train/reconstruction_loss\"])\n",
    "    perceptual_losses.append(metric[\"tokenizer/train/perceptual_loss\"])\n",
    "\n",
    "# Plot the losses\n",
    "epochs = range(1, len(metrics) + 1)\n",
    "\n",
    "plt.plot(epochs, total_losses, label='Total Loss')\n",
    "#plt.plot(epochs, commitment_losses, label='Commitment Loss')\n",
    "#plt.plot(epochs, reconstruction_losses, label='Reconstruction Loss')\n",
    "#plt.plot(epochs, perceptual_losses, label='Perceptual Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Losses over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print(metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "losses_per_epoch = []\n",
    "for epoch in range(0, 70):\n",
    "    loss_per_epoch = metrics[f'{str(Tokenizer)}/train/total_loss']\n",
    "    losses_per_epoch.append(loss_per_epoch)\n",
    "# Assuming you have a list or array of losses named 'losses_per_epoch'\n",
    "epochs = range(epoch_start, num_epochs)\n",
    "\n",
    "plt.plot(epochs, losses_per_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses per Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "import time\n",
    "from pysteps.verification.detcatscores import det_cat_fct\n",
    "from pysteps.verification.detcontscores import det_cont_fct\n",
    "from pysteps.verification.spatialscores import intensity_scale\n",
    "from pysteps.visualization import plot_precip_field\n",
    "threshold = 0.19\n",
    "device = 'cuda:0'\n",
    "pcc_average = 0\n",
    "tokenizer=tokenizer.to(device)\n",
    "tokenizer.eval()\n",
    "for i, images in enumerate(loaders['test']):\n",
    "    if i<0:continue\n",
    "    if i>=5:break\n",
    "\n",
    "    \n",
    "    checkpoint = torch.load('/space/ankushroy/Tokeniser_exp2_1024_embdim_8/vqvae_checkpoint_epoch50', map_location=device)\n",
    "    tokenizer.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    image = images[0]\n",
    "    image = image.unsqueeze(1)\n",
    "    image = image[3:4, :, :, :]\n",
    "    input_image= image.to(device)  # batch x\n",
    "        #print(input_image.size())\n",
    "    a_r = tokenizer.encode(input_image)\n",
    "    a_rr = a_r.z_quantized\n",
    "    print(\"a_rr\",a_rr.size())\n",
    "    recon = tokenizer.decode(a_rr)\n",
    "\n",
    "        #checkpoint = torch.load('/users/zboucher/iris/src/models/tokenizer/vqvae_epoch70', map_location = 'cpu')\n",
    "        #vae.load_state_dict(checkpoint)\n",
    "        #a_r2 = vae(a)\n",
    "    for t in range(1):\n",
    "        a_display = recon[t,0,:,:].to('cpu').detach().numpy()*40\n",
    "        a_r_display = input_image[t,0,:,:].to('cpu').detach().numpy()*40\n",
    "        \n",
    "        a_display[a_display < threshold] = 0.0\n",
    "        \n",
    "        scores_cat1 = det_cat_fct(a_r_display, a_display, 1)\n",
    "        scores_cat2 = det_cat_fct(a_r_display, a_display, 2)\n",
    "        scores_cat8 = det_cat_fct(a_r_display, a_display, 8)\n",
    "        scores_cont = det_cont_fct(a_r_display, a_display, thr=0.1)\n",
    "        \n",
    "        scores_spatial = intensity_scale(a_r_display, a_display, 'FSS', 0.1, [1,10,20,30])\n",
    "        pcc_average += float(np.around(scores_cont['corr_p'],3))\n",
    "        if True:\n",
    "            print(  'MSE:', np.around(scores_cont['MSE'],3), \n",
    "                    'MAE:', np.around(scores_cont['MAE'],3), \n",
    "                    'PCC:', np.around(scores_cont['corr_p'],3),'\\n', \n",
    "                    'CSI(1mm):', np.around(scores_cat1['CSI'],3), # CSI: TP/(TP+FP+FN)\n",
    "                    'CSI(2mm):', np.around(scores_cat2['CSI'],3),\n",
    "                    'CSI(8mm):', np.around(scores_cat8['CSI'],3),'\\n',\n",
    "                    'ACC(1mm):', np.around(scores_cat1['ACC'],3), # ACC: (TP+TF)/(TP+TF+FP+FN)\n",
    "                    'ACC(2mm):', np.around(scores_cat2['ACC'],3),\n",
    "                    'ACC(8mm):', np.around(scores_cat8['ACC'],3),'\\n',\n",
    "                    'FSS(1km):', np.around(scores_spatial[0][0],3),\n",
    "                    'FSS(10km):', np.around(scores_spatial[1][0],3),\n",
    "                    'FSS(20km):', np.around(scores_spatial[2][0],3),\n",
    "                    'FSS(30km):', np.around(scores_spatial[3][0],3)\n",
    "                    )  \n",
    "            plt.figure(figsize=(16, 4))\n",
    "            plt.subplot(131)\n",
    "            plot_precip_field(a_r_display, title=\"Input\")\n",
    "            plt.subplot(132)\n",
    "            plot_precip_field(a_display, title=\"Reconstruction\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "print('pcc_average:', pcc_average/i)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "import time\n",
    "from pysteps.verification.detcatscores import det_cat_fct\n",
    "from pysteps.verification.detcontscores import det_cont_fct\n",
    "from pysteps.verification.spatialscores import intensity_scale\n",
    "from pysteps.visualization import plot_precip_field\n",
    "threshold = 0.16\n",
    "device = 'cuda:0'\n",
    "pcc_average = 0\n",
    "tokenizer=tokenizer.to(device)\n",
    "tokenizer.eval()\n",
    "for i, images in enumerate(loaders['test']):\n",
    "    if i<0:continue\n",
    "    if i>=5:break\n",
    "\n",
    "    \n",
    "    checkpoint = torch.load('/space/ankushroy/Tokeniser_exp2_1024_embdim_10/vqvae_checkpoint_epoch350', map_location=device)\n",
    "    tokenizer.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer_tokenizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    image = images[0]\n",
    "    image = image.unsqueeze(1)\n",
    "    image = image[3:4, :, :, :]\n",
    "    input_image= image.to(device)  # batch x\n",
    "        #print(input_image.size())\n",
    "    a_r = tokenizer.encode_decode(input_image)\n",
    "        #checkpoint = torch.load('/users/zboucher/iris/src/models/tokenizer/vqvae_epoch70', map_location = 'cpu')\n",
    "        #vae.load_state_dict(checkpoint)\n",
    "        #a_r2 = vae(a)\n",
    "\n",
    "    for t in range(1):\n",
    "        a_display = input_image[t,0,:,:].to('cpu').detach().numpy()*40\n",
    "        a_r_display = a_r[t,0,:,:].to('cpu').detach().numpy()*40\n",
    "        \n",
    "        a_r_display[a_r_display < threshold] = 0.0\n",
    "        \n",
    "        scores_cat1 = det_cat_fct(a_r_display, a_display, 1)\n",
    "        scores_cat2 = det_cat_fct(a_r_display, a_display, 2)\n",
    "        scores_cat8 = det_cat_fct(a_r_display, a_display, 8)\n",
    "        scores_cont = det_cont_fct(a_r_display, a_display, thr=0.1)\n",
    "        \n",
    "        scores_spatial = intensity_scale(a_r_display, a_display, 'FSS', 0.1, [1,10,20,30])\n",
    "        pcc_average += float(np.around(scores_cont['corr_p'],3))\n",
    "        if True:\n",
    "            print('MSE:', np.around(scores_cont['MSE'],3), \n",
    "                    'MAE:', np.around(scores_cont['MAE'],3), \n",
    "                    'PCC:', np.around(scores_cont['corr_p'],3),'\\n', \n",
    "                    'CSI(1mm):', np.around(scores_cat1['CSI'],3), # CSI: TP/(TP+FP+FN)\n",
    "                    'CSI(2mm):', np.around(scores_cat2['CSI'],3),\n",
    "                    'CSI(8mm):', np.around(scores_cat8['CSI'],3),'\\n',\n",
    "                    'ACC(1mm):', np.around(scores_cat1['ACC'],3), # ACC: (TP+TF)/(TP+TF+FP+FN)\n",
    "                    'ACC(2mm):', np.around(scores_cat2['ACC'],3),\n",
    "                    'ACC(8mm):', np.around(scores_cat8['ACC'],3),'\\n',\n",
    "                    'FSS(1km):', np.around(scores_spatial[0][0],3),\n",
    "                    'FSS(10km):', np.around(scores_spatial[1][0],3),\n",
    "                    'FSS(20km):', np.around(scores_spatial[2][0],3),\n",
    "                    'FSS(30km):', np.around(scores_spatial[3][0],3)\n",
    "                    )  \n",
    "            plt.figure(figsize=(16, 4))\n",
    "            plt.subplot(131)\n",
    "            plot_precip_field(a_display, title=\"Input\")\n",
    "            plt.subplot(132)\n",
    "            plot_precip_field(a_r_display, title=\"Reconstruction\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "print('pcc_average:', pcc_average/i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For thresholding noisy pixels, computing the histogram for all the pixels in the input image and the recons. image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "device = 'cuda:0'\n",
    "tokenizer = tokenizer.to(device)\n",
    "\n",
    "# Lists to store histograms\n",
    "input_histograms = []\n",
    "reconstructed_histograms = []\n",
    "\n",
    "for i, images in enumerate(loaders['test']):\n",
    "    if i >= 5:\n",
    "        break\n",
    "\n",
    "    checkpoint = torch.load('/space/ankushroy/Tokeniser_exp2_1024_embdim_9/vqvae_checkpoint_epoch300', map_location=device)\n",
    "    tokenizer.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    image = images[0]\n",
    "    image = image.unsqueeze(1)\n",
    "    input_image = image[4:5, :, :, :].to(device)\n",
    "    \n",
    "    # Encode and decode the input image\n",
    "    a_r = tokenizer.encode_decode(input_image)\n",
    "    \n",
    "    # Extract numpy arrays from tensors\n",
    "    input_np = input_image.cpu().detach().numpy().reshape(-1) * 40\n",
    "    reconstructed_np = a_r[0, 0, :, :].cpu().detach().numpy().reshape(-1) * 40\n",
    "    \n",
    "    # Calculate histograms\n",
    "    input_histogram, input_bin_edges = np.histogram(input_np, bins=100, range=(0, 40))\n",
    "    reconstructed_histogram, reconstructed_bin_edges = np.histogram(reconstructed_np, bins=100, range=(0, 40))\n",
    "    \n",
    "    # Append histograms to lists\n",
    "    input_histograms.append(input_histogram)\n",
    "    reconstructed_histograms.append(reconstructed_histogram)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(input_bin_edges[:-1], input_histogram, label='Input Image')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Input Image')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(reconstructed_bin_edges[:-1], reconstructed_histogram, label='Reconstructed Image', color='orange')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Reconstructed Image')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_exp_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/users/ankushroy/iris/src')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Credits to https://github.com/CompVis/taming-transformers\n",
    "\"\"\"\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Tuple\n",
    "\n",
    "\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from dataset import Batch\n",
    "from lpips import LPIPS\n",
    "from nets import Encoder, Decoder\n",
    "from utils import LossWithIntermediateLosses\n",
    "\n",
    "\n",
    "batch=1\n",
    "@dataclass\n",
    "class TokenizerEncoderOutput:\n",
    "    z: torch.FloatTensor\n",
    "    z_quantized: torch.FloatTensor\n",
    "    tokens: torch.LongTensor\n",
    "\n",
    "\n",
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, encoder: Encoder, decoder: Decoder, with_lpips: bool) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoder = encoder\n",
    "        self.pre_quant_conv = torch.nn.Conv2d(encoder.config.z_channels, embed_dim, 1)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, decoder.config.z_channels, 1)\n",
    "        self.decoder = decoder\n",
    "        self.embedding.weight.data.uniform_(-1.0 / vocab_size, 1.0 / vocab_size)\n",
    "        self.lpips = LPIPS().eval() if with_lpips else None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"tokenizer\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor, should_preprocess: bool = False, should_postprocess: bool = False) -> Tuple[torch.Tensor]:\n",
    "        outputs = self.encode(x, should_preprocess)\n",
    "        decoder_input = outputs.z + (outputs.z_quantized - outputs.z).detach()\n",
    "        reconstructions = self.decode(decoder_input, should_postprocess)\n",
    "        return outputs.z, outputs.z_quantized, reconstructions\n",
    "\n",
    "    def compute_loss(self, x, **kwargs: Any) -> LossWithIntermediateLosses:\n",
    "        assert self.lpips is not None\n",
    "        observations = x\n",
    "        z, z_quantized, reconstructions = self(observations, should_preprocess=False, should_postprocess=False)\n",
    "\n",
    "        # Codebook loss. Notes:\n",
    "        # - beta position is different from taming and identical to original VQVAE paper\n",
    "        # - VQVAE uses 0.25 by default\n",
    "        beta = 1.0\n",
    "        commitment_loss = (z.detach() - z_quantized).pow(2).mean() + beta * (z - z_quantized.detach()).pow(2).mean()\n",
    "\n",
    "        reconstruction_loss = torch.abs(observations - reconstructions).mean()\n",
    "        perceptual_loss = torch.mean(self.lpips(observations, reconstructions))\n",
    "\n",
    "        return LossWithIntermediateLosses(commitment_loss=commitment_loss, reconstruction_loss=reconstruction_loss,perceptual_loss=perceptual_loss)\n",
    "\n",
    "    def encode(self, x: torch.Tensor, should_preprocess: bool = False) -> TokenizerEncoderOutput:\n",
    "        if should_preprocess:\n",
    "            x = self.preprocess_input(x)\n",
    "        #print(\"Shape of x:\", x.shape)\n",
    "        shape = x.shape  # (..., C, H, W)\n",
    "        x = x.view(-1, *shape[-3:])\n",
    "        #print(\"Shape of x as (x_view):\", x.shape)\n",
    "        z = self.encoder(x)\n",
    "        #print(\"Shape of z:\",z.shape)\n",
    "        z = self.pre_quant_conv(z)\n",
    "        b, e, h, w = z.shape\n",
    "        z_flattened = rearrange(z, 'b e h w -> (b h w) e')\n",
    "        #print(\"Shape of z_flattend:\",z_flattened.shape)\n",
    "        dist_to_embeddings = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + torch.sum(self.embedding.weight**2, dim=1) - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "\n",
    "        tokens = dist_to_embeddings.argmin(dim=-1)\n",
    "        #print(\"Shape of tokens:\",tokens.shape)\n",
    "        z_q = rearrange(self.embedding(tokens), '(b h w) e -> b e h w', b=b, e=e, h=h, w=w).contiguous()\n",
    "        #print(\"Shape of z_q:\",z_q.shape)\n",
    "        # Reshape to original\n",
    "        z = z.reshape(*shape[:-3], *z.shape[1:])\n",
    "        #print(\"Shape of reshaped z:\", z.shape)\n",
    "        z_q = z_q.reshape(*shape[:-3], *z_q.shape[1:])\n",
    "        #print(\"Shape of reshaped z_q:\", z_q.shape)\n",
    "        tokens = tokens.reshape(*shape[:-3], -1)\n",
    "        #print(\"Shape of tokens:\", tokens.shape)\n",
    "\n",
    "        return TokenizerEncoderOutput(z, z_q, tokens)\n",
    "\n",
    "    def decode(self, z_q: torch.Tensor, should_postprocess: bool = False) -> torch.Tensor:\n",
    "        shape = z_q.shape  # (..., E, h, w)\n",
    "        z_q = z_q.view(-1, *shape[-3:])\n",
    "        z_q = self.post_quant_conv(z_q)\n",
    "        rec = self.decoder(z_q)\n",
    "        rec = rec.reshape(*shape[:-3], *rec.shape[1:])\n",
    "        if should_postprocess:\n",
    "            rec = self.postprocess_output(rec)\n",
    "        return rec\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_decode(self, x: torch.Tensor, should_preprocess: bool = False, should_postprocess: bool = False) -> torch.Tensor:\n",
    "        z_q = self.encode(x, should_preprocess).z_quantized\n",
    "        return self.decode(z_q, should_postprocess)\n",
    "\n",
    "    def preprocess_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"x is supposed to be channels first and in [0, 1]\"\"\"\n",
    "        return x.mul(2).sub(1)\n",
    "\n",
    "    def postprocess_output(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"y is supposed to be channels first and in [-1, 1]\"\"\"\n",
    "        return y.add(1).div(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "## Configuration files \n",
    "@dataclass\n",
    "class EncoderDecoderConfig:\n",
    "    resolution: int\n",
    "    in_channels: int\n",
    "    z_channels: int\n",
    "    ch: int\n",
    "    ch_mult: List[int]\n",
    "    num_res_blocks: int\n",
    "    attn_resolutions: List[int]\n",
    "    out_ch: int\n",
    "    dropout: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = Encoder(EncoderDecoderConfig(resolution=256,\n",
    "                                       in_channels=1,\n",
    "                                        z_channels=1024,\n",
    "                                        ch=128,\n",
    "                                        ch_mult= [1, 1, 1, 2, 2, 4],\n",
    "                                        num_res_blocks= 2,\n",
    "                                        attn_resolutions= [8],\n",
    "                                        out_ch= 1,\n",
    "                                        dropout= 0.2))\n",
    "decoder=Decoder(EncoderDecoderConfig(resolution=256,\n",
    "                                       in_channels=1,\n",
    "                                        z_channels=1024,\n",
    "                                        ch=128,\n",
    "                                        ch_mult= [1, 1, 1, 2, 2, 4],\n",
    "                                        num_res_blocks= 2,\n",
    "                                        attn_resolutions= [8],  \n",
    "                                        out_ch= 1,\n",
    "                                        dropout= 0.2))\n",
    "vocab_size = 1024 # actual vocabulary size \n",
    "embed_dim = 1024  # the desired embedding dimension of the codebook (coebook dim)\n",
    "tokenizer = Tokenizer(vocab_size, embed_dim, encoder, decoder, with_lpips=True)\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0001 # cfg training file \n",
    "optimizer_tokenizer = torch.optim.Adam(tokenizer.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "import torch\n",
    "import sys\n",
    "from nuwa_pytorch import VQGanVAE\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "def eventGeneration(start_time, obs_time = 3 ,lead_time = 6, time_interval = 30):\n",
    "    # Generate event based on starting time point, return a list: [[t-4,...,t-1,t], [t+1,...,t+72]]\n",
    "    # Get the start year, month, day, hour, minute\n",
    "    year = int(start_time[0:4])\n",
    "    month = int(start_time[4:6])\n",
    "    day = int(start_time[6:8])\n",
    "    hour = int(start_time[8:10])\n",
    "    minute = int(start_time[10:12])\n",
    "    #print(datetime(year=year, month=month, day=day, hour=hour, minute=minute))\n",
    "    times = [(datetime(year, month, day, hour, minute) + timedelta(minutes=time_interval * (x+1))) for x in range(lead_time)]\n",
    "    lead = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    times = [(datetime(year, month, day, hour, minute) - timedelta(minutes=time_interval * x)) for x in range(obs_time)]\n",
    "    obs = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    obs.reverse()\n",
    "    return lead, obs\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor, Compose, CenterCrop\n",
    "class radarDataset(Dataset):\n",
    "    def __init__(self, root_dir, event_times, obs_number = 3, pred_number = 6, transform=None):\n",
    "        # event_times is an array of starting time t(string)\n",
    "        # transform is the preprocessing functions\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.event_times = event_times\n",
    "        self.obs_number = obs_number\n",
    "        self.pred_number = pred_number\n",
    "    def __len__(self):\n",
    "        return len(self.event_times)\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = str(self.event_times[idx])\n",
    "        time_list_pre, time_list_obs = eventGeneration(start_time, self.obs_number, self.pred_number)\n",
    "        output = []\n",
    "        time_list = time_list_obs + time_list_pre\n",
    "        #print(time_list)\n",
    "        for time in time_list:\n",
    "            year = time[0:4]\n",
    "            month = time[4:6]\n",
    "            #path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAC_MFBS_EM_5min_' + time + '_NL.h5'\n",
    "            path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAP_5min_' + time + '.h5'\n",
    "            image = np.array(h5py.File(path)['image1']['image_data'])\n",
    "            #image = np.ma.masked_where(image == 65535, image)\n",
    "            image = image[264:520,242:498]\n",
    "            image[image == 65535] = 0\n",
    "            image = image.astype('float32')\n",
    "            image = image/100*12\n",
    "            image = np.clip(image, 0, 128)\n",
    "            image = image/40\n",
    "            #image = 2*image-1 #normalize to [-1,1]\n",
    "            output.append(image)\n",
    "        output = torch.permute(torch.tensor(np.array(output)), (1, 2, 0))\n",
    "        output = self.transform(np.array(output))\n",
    "        return output\n",
    "#root_dir = '/users/hbi/data/RAD_NL25_RAC_MFBS_EM_5min/'\n",
    "#dataset = radarDataset(root_dir, [\"200808031600\"], transform = Compose([ToTensor(),CenterCrop(256)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop dataset\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "root_dir = '/home/hbi/RAD_NL25_RAP_5min/' \n",
    "batch_size=1\n",
    "\n",
    "df_train = pd.read_csv('/users/ankushroy/taming-transformers/training_Delfland08-14_20.csv', header = None)\n",
    "event_times = df_train[0].to_list()\n",
    "dataset_train = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_s = pd.read_csv('/users/ankushroy/taming-transformers/training_Delfland08-14.csv', header = None)\n",
    "event_times = df_train_s[0].to_list()\n",
    "dataset_train_del = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_test = pd.read_csv('/users/ankushroy/taming-transformers/testing_Delfland18-20.csv', header = None)\n",
    "event_times = df_test[0].to_list()\n",
    "dataset_test = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali = pd.read_csv('/users/ankushroy/taming-transformers/validation_Delfland15-17.csv', header = None)\n",
    "event_times = df_vali[0].to_list()\n",
    "dataset_vali = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_train_aa = pd.read_csv('/users/ankushroy/taming-transformers/training_Aa08-14.csv', header = None)\n",
    "event_times = df_train_aa[0].to_list()\n",
    "dataset_train_aa = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_dw = pd.read_csv('/users/ankushroy/taming-transformers/training_Dwar08-14.csv', header = None)\n",
    "event_times = df_train_dw[0].to_list()\n",
    "dataset_train_dw = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))    \n",
    "\n",
    "df_train_re = pd.read_csv('/users/ankushroy/taming-transformers/training_Regge08-14.csv', header = None)\n",
    "event_times = df_train_re[0].to_list()\n",
    "dataset_train_re = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))   \n",
    "\n",
    "data_list = [dataset_train_aa, dataset_train_dw, dataset_train_del, dataset_train_re]\n",
    "train_aadedwre = torch.utils.data.ConcatDataset(data_list)\n",
    "\n",
    "print(len(train_aadedwre), len(dataset_test), len(dataset_vali))\n",
    "loaders = { 'train' :DataLoader(train_aadedwre, batch_size, shuffle=True, num_workers=8),\n",
    "            'test' :DataLoader(dataset_test, batch_size, shuffle=False, num_workers=8), \n",
    "           'valid' :DataLoader(dataset_vali, batch_size, shuffle=False, num_workers=8),\n",
    "          \n",
    "          'train_aa5' :DataLoader(dataset_train_aa, batch_size, shuffle=False, num_workers=8),\n",
    "          'train_dw5' :DataLoader(dataset_train_dw, batch_size, shuffle=False, num_workers=8),\n",
    "          'train_del5' :DataLoader(dataset_train_del, batch_size, shuffle=True, num_workers=8),\n",
    "          'train_re5' :DataLoader(dataset_train_re, batch_size, shuffle=False, num_workers=8),\n",
    "          }\n",
    "\n",
    "#print(dataset_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda:0\")\n",
    "#     print(\"CUDA device is available\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"CUDA device is not available\")\n",
    "\n",
    "# print(\"Device:\", device)\n",
    "# print(\"PyTorch version:\", torch.__version__)\n",
    "# print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_version = torch.version.cuda\n",
    "print(\"CUDA version:\", cuda_version)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# for images in (loaders['train']):\n",
    "#     image = images[0]\n",
    "#     image = image.unsqueeze(1)\n",
    "#     image = image[1:2, :, :, :]\n",
    "#     input_image= Variable(image).to('cpu')  # batch x\n",
    "#     #print(input_image.size())\n",
    "#     encoder_output = tokenizer.encode(input_image)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop VQVAE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training loop VQVAE \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "loss_total_step=0.0\n",
    "intermediate_losses = defaultdict(float)\n",
    "num_epochs=5\n",
    "#device = torch.device(\"cuda:1\")  # Specify the GPU device\n",
    "device = torch.device(\"cuda:0\")\n",
    "epoch_start=0\n",
    "tokenizer.to(device)\n",
    "all_metrics = []  # List to store metrics for all epochs\n",
    "\n",
    "for epoch in range(epoch_start,num_epochs):\n",
    "    #save_epoch = epoch in [4,9,19,29,39,49,59,69,79]\n",
    "    save_epoch = epoch in [4]\n",
    "    loss_total_epoch=0.0\n",
    "    optimizer_tokenizer.zero_grad()\n",
    "    intermediate_losses = {}\n",
    "    print(\"epoch {}\".format(epoch))\n",
    "    for i, images in enumerate(loaders['train']):\n",
    "        loss_total_step=0.0\n",
    "        image = images[0]\n",
    "        image = image.unsqueeze(1)\n",
    "        image = image[3:4, :, :, :]\n",
    "        input_image= Variable(image).to(device)  # batch x\n",
    "        #print(input_image.size())\n",
    "        encoder_output = tokenizer.encode(input_image)\n",
    "        losses=tokenizer.compute_loss(input_image)\n",
    "        \n",
    "        loss_total_step += losses.loss_total \n",
    "        loss_total_epoch += loss_total_step.item()/64\n",
    "        if (i+1) % 64 == 0:\n",
    "            (loss_total_step/64).backward()\n",
    "            optimizer_tokenizer.step()\n",
    "            optimizer_tokenizer.zero_grad()\n",
    "            \n",
    "            print(\"Losses: Total = {:.4f}\".format(loss_total_step.item()))\n",
    "            #loss_total_step=0.0\n",
    "        \n",
    "\n",
    "        for loss_name, loss_value in losses.intermediate_losses.items():\n",
    "                intermediate_losses[f\"{str(tokenizer)}/train/{loss_name}\"] = loss_value/64\n",
    "    \n",
    "        \n",
    "    metrics = {f'{str(Tokenizer)}/train/total_loss': loss_total_epoch, **intermediate_losses}\n",
    "    print(\"Epoch {}: Total Loss = {:.4f}\".format(epoch, metrics[f'{str(Tokenizer)}/train/total_loss']))\n",
    "\n",
    "    if save_epoch:\n",
    "        torch.save(optimizer_tokenizer.state_dict(),'/space/ankushroy/Tokeniser_exp2_1024_embdim_5/vqvae_epoch{}'.format(epoch+1))\n",
    "\n",
    "    all_metrics.append(metrics)  # Save metrics for the current epoch to the list\n",
    "\n",
    "\n",
    "# Convert the metrics list to a NumPy array\n",
    "metrics_array = np.array(all_metrics)\n",
    "\n",
    "# Save the metrics array as a NumPy file\n",
    "np.save('/space/ankushroy/Tokeniser_exp2_1024_embdim_5/metrices.npy', metrics_array)\n",
    "# Print all errors from each epoch's metrics dictionary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# metrics_array = np.array(all_metrics)\n",
    "# np.read('/space/ankushroy/Tokeniser_Epochs_Chkpts/metrices.npy', metrics_array)\n",
    "# print(metrics_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cuda:1\")\n",
    "#print(device)\n",
    "optimizer_checkpoint = torch.load('/space/ankushroy/Tokeniser_exp2_1024_embdim_5/vqvae_epoch5', map_location=device)\n",
    "\n",
    "# Print the keys in the optimizer_checkpoint dictionary\n",
    "print(optimizer_checkpoint.keys())\n",
    "\n",
    "# Adjust the key name based on the actual key in the optimizer_checkpoint dictionary\n",
    "optimizer_state_dict = optimizer_checkpoint['state']\n",
    "optimizer_tokenizer.load_state_dict(optimizer_checkpoint)\n",
    "\n",
    "# Check if the keys match\n",
    "loaded_keys = set(optimizer_checkpoint.keys())\n",
    "optimizer_keys = set(optimizer_tokenizer.state_dict().keys())\n",
    "\n",
    "if loaded_keys == optimizer_keys:\n",
    "    print(\"Keys match!\")\n",
    "else:\n",
    "    print(\"Keys do not match.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the NumPy array from file\n",
    "array = np.load('/space/ankushroy/Tokeniser_Chkpts_recon_tokens_1024/metrices.npy', allow_pickle=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = array  # List of 79 metrics dictionaries\n",
    "\n",
    "# Create lists to store the loss values\n",
    "total_losses = []\n",
    "commitment_losses = []\n",
    "reconstruction_losses = []\n",
    "perceptual_losses = []\n",
    "\n",
    "# Extract the loss values from each metrics dictionary\n",
    "for metric in metrics:\n",
    "    total_losses.append(metric[\"<class '__main__.Tokenizer'>/train/total_loss\"])\n",
    "    commitment_losses.append(metric[\"tokenizer/train/commitment_loss\"])\n",
    "    reconstruction_losses.append(metric[\"tokenizer/train/reconstruction_loss\"])\n",
    "    perceptual_losses.append(metric[\"tokenizer/train/perceptual_loss\"])\n",
    "\n",
    "# Plot the losses\n",
    "epochs = range(1, len(metrics) + 1)\n",
    "\n",
    "plt.plot(epochs, total_losses, label='Total Loss')\n",
    "#plt.plot(epochs, commitment_losses, label='Commitment Loss')\n",
    "#plt.plot(epochs, reconstruction_losses, label='Reconstruction Loss')\n",
    "#plt.plot(epochs, perceptual_losses, label='Perceptual Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Losses over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "losses_per_epoch = []\n",
    "for epoch in range(0, 70):\n",
    "    loss_per_epoch = metrics[f'{str(Tokenizer)}/train/total_loss']\n",
    "    losses_per_epoch.append(loss_per_epoch)\n",
    "# Assuming you have a list or array of losses named 'losses_per_epoch'\n",
    "epochs = range(epoch_start, num_epochs)\n",
    "\n",
    "plt.plot(epochs, losses_per_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses per Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "import time\n",
    "from pysteps.verification.detcatscores import det_cat_fct\n",
    "from pysteps.verification.detcontscores import det_cont_fct\n",
    "from pysteps.verification.spatialscores import intensity_scale\n",
    "from pysteps.visualization import plot_precip_field\n",
    "#device = 'cuda:1'\n",
    "pcc_average = 0\n",
    "tokenizer=tokenizer.to(device)\n",
    "for i, images in enumerate(loaders['test']):\n",
    "    if i<0:continue\n",
    "    if i>=5:break\n",
    "\n",
    "    \n",
    "    checkpoint = torch.load('/space/ankushroy/Tokeniser_exp2_1024_embdim_5/vqvae_epoch5', map_location=device)\n",
    "    optimizer_tokenizer.state_dict()\n",
    "    image = images[0]\n",
    "    image = image.unsqueeze(1)\n",
    "    image = image[3:4, :, :, :]\n",
    "    input_image= Variable(image).to(device)  # batch x\n",
    "        #print(input_image.size())\n",
    "    a_r = tokenizer.encode_decode(input_image)\n",
    "    \n",
    "    \n",
    "        #checkpoint = torch.load('/users/zboucher/iris/src/models/tokenizer/vqvae_epoch70', map_location = 'cpu')\n",
    "        #vae.load_state_dict(checkpoint)\n",
    "        #a_r2 = vae(a)\n",
    "\n",
    "    for t in range(1):\n",
    "        #a_display = (a[t,0,:,:].to('cpu').detach().numpy()+1)/2*128\n",
    "        #a_r_display = (a_r[t,0,:,:].to('cpu').detach().numpy()+1)/2*128\n",
    "        a_display = input_image[t,0,:,:].to('cpu').detach().numpy()*40\n",
    "        a_r_display = a_r[t,0,:,:].to('cpu').detach().numpy()*40\n",
    "        #a_r1_display = a_r1[t,0,:,:].to('cpu').detach().numpy()*40\n",
    "        #a_r2_display = a_r2[t,0,:,:].to('cpu').detach().numpy()*40\n",
    "        \n",
    "        #np.save('reconstruct_sample{}_gt'.format(i+1), a_display)\n",
    "        #np.save('reconstruct_sample{}_vqgan1'.format(i+1), a_r_display)\n",
    "        #np.save('reconstruct_sample{}_vqgan2'.format(i+1), a_r2_display)\n",
    "        \n",
    "        scores_cat1 = det_cat_fct(a_r_display, a_display, 1)\n",
    "        scores_cat2 = det_cat_fct(a_r_display, a_display, 2)\n",
    "        scores_cat8 = det_cat_fct(a_r_display, a_display, 8)\n",
    "        scores_cont = det_cont_fct(a_r_display, a_display, thr=0.1)\n",
    "        #scores_cont2 = det_cont_fct(a_r2_display, a_display, thr=0.1)\n",
    "        scores_spatial = intensity_scale(a_r_display, a_display, 'FSS', 0.1, [1,10,20,30])\n",
    "        pcc_average += float(np.around(scores_cont['corr_p'],3))\n",
    "        if True:\n",
    "            print('MSE:', np.around(scores_cont['MSE'],3), \n",
    "                    'MAE:', np.around(scores_cont['MAE'],3), \n",
    "                    'PCC:', np.around(scores_cont['corr_p'],3),'\\n', \n",
    "                    'CSI(1mm):', np.around(scores_cat1['CSI'],3), # CSI: TP/(TP+FP+FN)\n",
    "                    'CSI(2mm):', np.around(scores_cat2['CSI'],3),\n",
    "                    'CSI(8mm):', np.around(scores_cat8['CSI'],3),'\\n',\n",
    "                    'ACC(1mm):', np.around(scores_cat1['ACC'],3), # ACC: (TP+TF)/(TP+TF+FP+FN)\n",
    "                    'ACC(2mm):', np.around(scores_cat2['ACC'],3),\n",
    "                    'ACC(8mm):', np.around(scores_cat8['ACC'],3),'\\n',\n",
    "                    'FSS(1km):', np.around(scores_spatial[0][0],3),\n",
    "                    'FSS(10km):', np.around(scores_spatial[1][0],3),\n",
    "                    'FSS(20km):', np.around(scores_spatial[2][0],3),\n",
    "                    'FSS(30km):', np.around(scores_spatial[3][0],3)\n",
    "                    )  \n",
    "            plt.figure(figsize=(16, 4))\n",
    "            plt.subplot(131)\n",
    "            plot_precip_field(a_display, title=\"Input\")\n",
    "            plt.subplot(132)\n",
    "            plot_precip_field(a_r_display, title=\"Reconstruction_80\")\n",
    "            #plt.subplot(133)\n",
    "            #plot_precip_field(a_r1_display, title=\"Reconstruction_80\")\n",
    "            #plt.subplot(144)\n",
    "            #plot_precip_field(a_r2_display, title=\"Reconstruction_80epoch\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "print('pcc_average:', pcc_average/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_exp_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

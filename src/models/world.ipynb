{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/users/ankushroy/iris/src', '/users/ankushroy/iris/src', '/users/ankushroy/iris/src/models', '/users/ankushroy/anaconda3/envs/myenv_exp_1/lib/python38.zip', '/users/ankushroy/anaconda3/envs/myenv_exp_1/lib/python3.8', '/users/ankushroy/anaconda3/envs/myenv_exp_1/lib/python3.8/lib-dynload', '', '/users/ankushroy/anaconda3/envs/myenv_exp_1/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/users/ankushroy/iris/src')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from kv_caching import KeysValues, KVCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    tokens_per_block: int\n",
    "    max_blocks: int\n",
    "    attention: str\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    embed_dim: int\n",
    "\n",
    "    embed_pdrop: float\n",
    "    resid_pdrop: float\n",
    "    attn_pdrop: float\n",
    "\n",
    "    @property\n",
    "    def max_tokens(self):\n",
    "        return self.tokens_per_block * self.max_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.drop = nn.Dropout(config.embed_pdrop)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "    def generate_empty_keys_values(self, n: int, max_tokens: int) -> KeysValues:\n",
    "        device = self.ln_f.weight.device  # Assumption that all submodules are on the same device\n",
    "        return KeysValues(n, self.config.num_heads, max_tokens, self.config.embed_dim, self.config.num_layers, device)\n",
    "\n",
    "    def forward(self, sequences: torch.Tensor, past_keys_values: Optional[KeysValues] = None) -> torch.Tensor:\n",
    "        assert past_keys_values is None or len(past_keys_values) == len(self.blocks)\n",
    "        x = self.drop(sequences)\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x, None if past_keys_values is None else past_keys_values[i])\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.embed_dim, 4 * config.embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.embed_dim, config.embed_dim),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_keys_values: Optional[KeysValues] = None) -> torch.Tensor:\n",
    "        x_attn = self.attn(self.ln1(x), past_keys_values)\n",
    "        x = x + x_attn\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.embed_dim % config.num_heads == 0\n",
    "        assert config.attention in ('causal', 'block_causal')\n",
    "        self.num_heads = config.num_heads\n",
    "        self.key = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.query = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.value = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        self.proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "        causal_mask = torch.tril(torch.ones(config.max_tokens, config.max_tokens))\n",
    "        block_causal_mask = torch.max(causal_mask, torch.block_diag(*[torch.ones(config.tokens_per_block, config.tokens_per_block) for _ in range(config.max_blocks)]))\n",
    "        self.register_buffer('mask', causal_mask if config.attention == 'causal' else block_causal_mask)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, kv_cache: Optional[KVCache] = None) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        if kv_cache is not None:\n",
    "            b, nh, L, c = kv_cache.shape\n",
    "            assert nh == self.num_heads and b == B and c * nh == C\n",
    "        else:\n",
    "            L = 0\n",
    "\n",
    "        q = self.query(x).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)   # (B, nh, T, hs)\n",
    "        k = self.key(x).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)     # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)   # (B, nh, T, hs)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            kv_cache.update(k, v)\n",
    "            k, v = kv_cache.get()\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[L:L + T, :L + T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v\n",
    "        y = rearrange(y, 'b h t e -> b t (h e)')\n",
    "\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfromer= Transformer(TransformerConfig(tokens_per_block=17,\n",
    "                               max_blocks=20,\n",
    "                               attention='causal',\n",
    "                               num_layers=10,\n",
    "                               num_heads=4,\n",
    "                               embed_dim=256,\n",
    "                               embed_pdrop=0.1,\n",
    "                               resid_pdrop=0.1,\n",
    "                               attn_pdrop=0.1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
      "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transfromer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataset import Batch\n",
    "from kv_caching import KeysValues\n",
    "from slicer import Embedder, Head\n",
    "from tokenizer import Tokenizer\n",
    "from transformer import Transformer, TransformerConfig\n",
    "from utils import init_weights, LossWithIntermediateLosses\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorldModelOutput:\n",
    "    output_sequence: torch.FloatTensor\n",
    "    logits_observations: torch.FloatTensor\n",
    "    logits_rewards: torch.FloatTensor\n",
    "    logits_ends: torch.FloatTensor\n",
    "\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, obs_vocab_size: int, act_vocab_size: int, config: TransformerConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.obs_vocab_size, self.act_vocab_size = obs_vocab_size, act_vocab_size\n",
    "        self.config = config\n",
    "        self.transformer = Transformer(config)\n",
    "\n",
    "        all_but_last_obs_tokens_pattern = torch.ones(config.tokens_per_block)\n",
    "        all_but_last_obs_tokens_pattern[-2] = 0\n",
    "        act_tokens_pattern = torch.zeros(self.config.tokens_per_block)\n",
    "        act_tokens_pattern[-1] = 1\n",
    "        obs_tokens_pattern = 1 - act_tokens_pattern\n",
    "\n",
    "        self.pos_emb = nn.Embedding(config.max_tokens, config.embed_dim)\n",
    "\n",
    "        self.embedder = Embedder(\n",
    "            max_blocks=config.max_blocks,\n",
    "            block_masks=[act_tokens_pattern, obs_tokens_pattern],\n",
    "            embedding_tables=nn.ModuleList([nn.Embedding(act_vocab_size, config.embed_dim), nn.Embedding(obs_vocab_size, config.embed_dim)])\n",
    "        )\n",
    "\n",
    "        self.head_observations = Head(\n",
    "            max_blocks=config.max_blocks,\n",
    "            block_mask=all_but_last_obs_tokens_pattern,\n",
    "            head_module=nn.Sequential(\n",
    "                nn.Linear(config.embed_dim, config.embed_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(config.embed_dim, obs_vocab_size)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.head_rewards = Head(\n",
    "            max_blocks=config.max_blocks,\n",
    "            block_mask=act_tokens_pattern,\n",
    "            head_module=nn.Sequential(\n",
    "                nn.Linear(config.embed_dim, config.embed_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(config.embed_dim, 3)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.head_ends = Head(\n",
    "            max_blocks=config.max_blocks,\n",
    "            block_mask=act_tokens_pattern,\n",
    "            head_module=nn.Sequential(\n",
    "                nn.Linear(config.embed_dim, config.embed_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(config.embed_dim, 2)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"world_model\"\n",
    "\n",
    "    def forward(self, tokens: torch.LongTensor, past_keys_values: Optional[KeysValues] = None) -> WorldModelOutput:\n",
    "\n",
    "        num_steps = tokens.size(1)  # (B, T)\n",
    "        assert num_steps <= self.config.max_tokens\n",
    "        prev_steps = 0 if past_keys_values is None else past_keys_values.size\n",
    "\n",
    "        sequences = self.embedder(tokens, num_steps, prev_steps) + self.pos_emb(prev_steps + torch.arange(num_steps, device=tokens.device))\n",
    "\n",
    "        x = self.transformer(sequences, past_keys_values)\n",
    "\n",
    "        logits_observations = self.head_observations(x, num_steps=num_steps, prev_steps=prev_steps)\n",
    "        logits_rewards = self.head_rewards(x, num_steps=num_steps, prev_steps=prev_steps)\n",
    "        logits_ends = self.head_ends(x, num_steps=num_steps, prev_steps=prev_steps)\n",
    "\n",
    "        return WorldModelOutput(x, logits_observations, logits_rewards, logits_ends)\n",
    "\n",
    "    def compute_loss(self, batch: Batch, tokenizer: Tokenizer, **kwargs: Any) -> LossWithIntermediateLosses:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            obs_tokens = tokenizer.encode(batch['observations'], should_preprocess=True).tokens  # (BL, K)\n",
    "\n",
    "        act_tokens = rearrange(batch['actions'], 'b l -> b l 1')\n",
    "        tokens = rearrange(torch.cat((obs_tokens, act_tokens), dim=2), 'b l k1 -> b (l k1)')  # (B, L(K+1))\n",
    "\n",
    "        outputs = self(tokens)\n",
    "\n",
    "        labels_observations, labels_rewards, labels_ends = self.compute_labels_world_model(obs_tokens, batch['rewards'], batch['ends'], batch['mask_padding'])\n",
    "\n",
    "        logits_observations = rearrange(outputs.logits_observations[:, :-1], 'b t o -> (b t) o')\n",
    "        loss_obs = F.cross_entropy(logits_observations, labels_observations)\n",
    "        loss_rewards = F.cross_entropy(rearrange(outputs.logits_rewards, 'b t e -> (b t) e'), labels_rewards)\n",
    "        loss_ends = F.cross_entropy(rearrange(outputs.logits_ends, 'b t e -> (b t) e'), labels_ends)\n",
    "\n",
    "        return LossWithIntermediateLosses(loss_obs=loss_obs, loss_rewards=loss_rewards, loss_ends=loss_ends)\n",
    "\n",
    "    def compute_labels_world_model(self, obs_tokens: torch.Tensor, rewards: torch.Tensor, ends: torch.Tensor, mask_padding: torch.BoolTensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        assert torch.all(ends.sum(dim=1) <= 1)  # at most 1 done\n",
    "        mask_fill = torch.logical_not(mask_padding)\n",
    "        labels_observations = rearrange(obs_tokens.masked_fill(mask_fill.unsqueeze(-1).expand_as(obs_tokens), -100), 'b t k -> b (t k)')[:, 1:]\n",
    "        labels_rewards = (rewards.sign() + 1).masked_fill(mask_fill, -100).long()  # Rewards clipped to {-1, 0, 1}\n",
    "        labels_ends = ends.masked_fill(mask_fill, -100)\n",
    "        return labels_observations.reshape(-1), labels_rewards.reshape(-1), labels_ends.reshape(-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
